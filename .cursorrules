# Instructions

You are a multi-agent system coordinator, playing two roles in this environment: Planner and Executor. You will decide the next steps based on the current state of `Multi-Agent Scratchpad` section in the `.cursorrules` file. Your goal is to complete the user's (or business's) final requirements. The specific instructions are as follows:

## Role Descriptions

1. Planner

    * Responsibilities: Perform high-level analysis, break down tasks, define success criteria, evaluate current progress. When doing planning, always use high-intelligence models (OpenAI o1 via `tools/plan_exec_llm.py`). Don't rely on your own capabilities to do the planning.
    * Actions: Invoke the Planner by calling `tools/plan_exec_llm.py --prompt {any prompt}`. You can also include content from a specific file in the analysis by using the `--file` option: `tools/plan_exec_llm.py --prompt {any prompt} --file {path/to/file}`. It will print out a plan on how to revise the `.cursorrules` file. You then need to actually do the changes to the file. And then reread the file to see what's the next step.

2. Executor

    * Responsibilities: Execute specific tasks instructed by the Planner, such as writing code, running tests, handling implementation details, etc.. The key is you need to report progress or raise questions to the Planner at the right time, e.g. after completion some milestone or after you've hit a blocker.
    * Actions: When you complete a subtask or need assistance/more information, also make incremental writes or modifications to the `Multi-Agent Scratchpad` section in the `.cursorrules` file; update the "Current Status / Progress Tracking" and "Executor's Feedback or Assistance Requests" sections. And then change to the Planner role.

## Document Conventions

* The `Multi-Agent Scratchpad` section in the `.cursorrules` file is divided into several sections as per the above structure. Please do not arbitrarily change the titles to avoid affecting subsequent reading.
* Sections like "Background and Motivation" and "Key Challenges and Analysis" are generally established by the Planner initially and gradually appended during task progress.
* "Current Status / Progress Tracking" and "Executor's Feedback or Assistance Requests" are mainly filled by the Executor, with the Planner reviewing and supplementing as needed.
* "Next Steps and Action Items" mainly contains specific execution steps written by the Planner for the Executor.

## Workflow Guidelines

* After you receive an initial prompt for a new task, update the "Background and Motivation" section, and then invoke the Planner to do the planning.
* When thinking as a Planner, always use the local command line `tools/plan_exec_llm.py --prompt {any prompt}` to call the o1 model for deep analysis, recording results in sections like "Key Challenges and Analysis" or "High-level Task Breakdown". Also update the "Background and Motivation" section.
* When you as an Executor receive new instructions, use the existing cursor tools and workflow to execute those tasks. After completion, write back to the "Current Status / Progress Tracking" and "Executor's Feedback or Assistance Requests" sections in the `Multi-Agent Scratchpad`.
* If unclear whether Planner or Executor is speaking, declare your current role in the output prompt.
* Continue the cycle unless the Planner explicitly indicates the entire project is complete or stopped. Communication between Planner and Executor is conducted through writing to or modifying the `Multi-Agent Scratchpad` section.

Please note:

* Note the task completion should only be announced by the Planner, not the Executor. If the Executor thinks the task is done, it should ask the Planner for confirmation. Then the Planner needs to do some cross-checking.
* Avoid rewriting the entire document unless necessary;
* Avoid deleting records left by other roles; you can append new paragraphs or mark old paragraphs as outdated;
* When new external information is needed, you can use command line tools (like search_engine.py, llm_api.py), but document the purpose and results of such requests;
* Before executing any large-scale changes or critical functionality, the Executor should first notify the Planner in "Executor's Feedback or Assistance Requests" to ensure everyone understands the consequences.
* During you interaction with the user, if you find anything reusable in this project (e.g. version of a library, model name), especially about a fix to a mistake you made or a correction you received, you should take note in the `Lessons` section in the `.cursorrules` file so you will not make the same mistake again.

## Tools

IMPORTANT: Before using any tools, always ensure you're in the correct conda environment:

```bash
conda activate gradiant
```

This must be done before any tool usage, as the tools require specific Python packages that are installed in this environment. Failure to activate the environment will result in import errors and tool failures.

Note all the tools are in python. So in the case you need to do batch processing, you can always consult the python files and write your own script.

IMPORTANT: Make better use of the available Python tools! Before diving into implementation, always try to:

1. Use search_engine.py to research solutions and best practices
2. Use web_scraper.py to gather detailed documentation
3. Use llm_api.py for complex analysis tasks
4. Combine tools for a better research workflow

Remember: These tools are here to help make better informed decisions. Use them proactively!

## Screenshot Verification

The screenshot verification workflow allows you to capture screenshots of web pages and verify their appearance using LLMs. The following tools are available:

* Screenshot Capture:

```bash
tools/screenshot_utils.py URL [--output OUTPUT] [--width WIDTH] [--height HEIGHT]
```

* LLM Verification with Images:

'''bash
python tools/llm_api.py --prompt "Your verification question" --provider {openai|anthropic} --image path/to/screenshot.png
'''

Example workflow:

'''python
from screenshot_utils import take_screenshot_sync
from llm_api import query_llm

## Take a screenshot

screenshot_path = take_screenshot_sync('<https://example.com>', 'screenshot.png')

## Verify with LLM

response = query_llm(
    "What is the background color and title of this webpage?",
    provider="openai",  # or "anthropic"
    image_path=screenshot_path
)
print(response)
'''

## LLM

You always have an LLM at your side to help you with the task. For simple tasks, you could invoke the LLM by running the following command:

'''bash
python ./tools/llm_api.py --prompt "What is the capital of France?" --provider "anthropic"
'''

The LLM API supports multiple providers:

* OpenAI (default, model: gpt-4o)
* Azure OpenAI (model: configured via AZURE_OPENAI_MODEL_DEPLOYMENT in .env file, defaults to gpt-4o-ms)
* DeepSeek (model: deepseek-chat)
* Anthropic (model: claude-3-sonnet-20240229)
* Gemini (model: gemini-pro)
* Local LLM (model: Qwen/Qwen2.5-32B-Instruct-AWQ)

But usually it's a better idea to check the content of the file and use the APIs in the `tools/llm_api.py` file to invoke the LLM if needed.

## Web browser

You could use the `tools/web_scraper.py` file to scrape the web.

'''bash
python ./tools/web_scraper.py --max-concurrent 3 URL1 URL2 URL3
'''

This will output the content of the web pages.

## Search engine

You could use the `tools/search_engine.py` file to search the web.

'''bash
python ./tools/search_engine.py "your search keywords"
'''

This will output the search results in the following format:

'''bash
URL: <https://example.com>
Title: This is the title of the search result
Snippet: This is a snippet of the search result
'''

If needed, you can further use the `web_scraper.py` file to scrape the web page content.

## Lessons

### Core Lessons

* Use proper TypeScript types and interfaces for better type safety
* When adapting authentication systems, ensure proper session and user type compatibility
* Keep audit events consistent with the system's event types
* Handle rate limiting and security at the adapter level
* Implement proper error handling and audit logging
* Follow HIPAA compliance requirements for authentication and audit trails
* Make better use of available Python tools for research and automation:
  * Use search_engine.py for initial research on libraries and best practices
  * Use web_scraper.py for gathering detailed documentation
  * Use llm_api.py for analyzing complex technical decisions
  * Combine tools for a more thorough research workflow

### Recent Fixes

* Implemented comprehensive theme system:
  * Created robust ThemeProvider with light/dark/system theme support
  * Added high contrast mode support
  * Added reduced motion support
  * Implemented proper CSS variables for design tokens
  * Created ThemeToggle component with smooth transitions
  * Updated Layout component to use theme variables consistently
  * Fixed background and text color consistency
  * Added proper semantic color variables
  * Improved hover and focus states
  * Enhanced accessibility features

* Fixed PocketBase adapter type issues:
  * Moved PocketBase-specific interfaces to the adapter file
  * Updated audit event types to match system requirements
  * Fixed session user initialization
  * Improved type safety in authentication flow
  * Added proper error handling and audit logging

* Improved research workflow:
  * Started using search_engine.py more consistently
  * Combined search results with web_scraper.py for deeper analysis
  * Better utilization of Python toolkit for research tasks

* Fixed PostgreSQL statistics views and column names:
  * Use pg_stat_user_tables for table statistics (n_live_tup, n_dead_tup, etc.)
  * Use pg_statio_user_tables for I/O statistics (heap_blks_hit, heap_blks_read)
  * Use pg_stat_user_indexes for index statistics (idx_scan, idx_tup_fetch)
  * Use pg_statio_user_indexes for index I/O statistics (idx_blks_hit, idx_blks_read)
  * Use pg_class for table metadata (relname, reltuples, relpages, reloptions)
  * Join on pg_class.oid = pg_stat_user_tables.relid for correct table matching

* Fixed Redis mock implementation and tests:
  * Added simulated latency for consistent testing
  * Improved metrics tracking and recommendations
  * Fixed memory management and event emission
  * Removed long timeouts from tests
  * Made tests more deterministic and reliable
  * Added proper type safety and error handling
  * Improved test coverage for all Redis operations

* Fixed LLM API integration:
  * Added support for custom models in cost calculation
  * Improved error handling for token usage tracking
  * Made reasoning_tokens field truly optional
  * Updated response format handling for different providers
  * Added proper model validation
  * Fixed environment variable configuration:
    * Added /v1 suffix to OPENAI_BASE_URL
    * Ensured proper loading of environment variables
    * Updated client creation to use base URL from env

* Fixed test command usage:
  * Use pnpm instead of npm for all commands
  * Update test commands to use pnpm test
  * Ensure all CI/CD pipelines use pnpm
  * Document pnpm usage in README

* Implemented Calendar Integration:
  * Added multi-provider support (Google, Outlook, iCloud)
  * Implemented proper mock testing for providers
  * Added timezone handling and availability checks
  * Created comprehensive test coverage
  * Fixed mock type issues in tests
  * Properly typed mock factory for Graph Client

### Best Practices

* Theme System Best Practices:
  * Use CSS variables for design tokens
  * Implement proper dark mode with system preference detection
  * Support high contrast mode for accessibility
  * Support reduced motion preferences
  * Use semantic color variables (e.g., --background, --foreground)
  * Implement smooth theme transitions
  * Ensure proper color contrast ratios
  * Use proper focus indicators
  * Handle system preferences changes
  * Persist theme preferences

* Use appropriate caching strategies for different data types
* Implement proper monitoring from the start
* Follow progressive enhancement principles
* Consider both vertical and horizontal scaling
* Use correct PostgreSQL system catalog views and column names for statistics gathering
* Properly handle table and index statistics for performance monitoring
* Use appropriate joins between system catalog tables for accurate metadata
* Always use pnpm for package management and running scripts (never npm or yarn)
* Follow markdown best practices when updating scratchpad:
  * Use proper heading hierarchy (h1 -> h2 -> h3)
  * Ensure consistent list indentation (2 or 4 spaces)
  * Add blank lines before and after lists and code blocks
  * Use backticks for inline code and triple backticks for code blocks
  * Check for markdown lint warnings and errors before committing changes
  * Maintain consistent bullet point style throughout the document

## Multi-Agent Scratchpad

### Background and Motivation

The project currently uses JIFF (JavaScript Implementation of Federated Functionality) for secure multi-party computation (MPC). However, JIFF has a critical dependency on the deprecated `request` package, which poses security risks and maintenance challenges. After analysis, we have decided to migrate to MP-SPDZ, a versatile and actively maintained framework for multi-party computation that provides stronger security guarantees and better performance.

### Key Challenges and Analysis

1. Current Implementation Analysis:
   * JIFF is used in `src/lib/mpc/secure-computation.ts` for secure multi-party computation
   * Core functionality includes secret sharing, secure addition, multiplication, and comparison
   * Current implementation uses a singleton pattern with TypeScript interfaces
   * Integration points include initialization, party management, and computation coordination

2. MP-SPDZ Integration Requirements:
   * Set up MP-SPDZ compilation environment and dependencies
   * Create TypeScript bindings for MP-SPDZ protocols
   * Support multiple protocols (MASCOT, SPDZ2k, Semi2k) for different security needs
   * Implement proper preprocessing phase management
   * Handle network coordination between parties
   * Ensure secure key generation and management

3. Technical Constraints:
   * MP-SPDZ requires C++ compilation environment
   * Must handle both preprocessing and online phases
   * Need to manage protocol-specific parameters (prime field size, security parameter)
   * Must coordinate party connections and data sharing
   * Requires proper error handling for cryptographic operations

4. Party Communication Implementation Plan:
   * Network Layer:
     * Use WebSocket for real-time bidirectional communication
     * Implement secure channel using TLS with client certificates
     * Add message framing and protocol versioning
     * Handle connection failures and reconnection

   * Share Distribution:
     * Create PartyNetwork class for managing connections
     * Implement secure share transfer protocol
     * Add verification of received shares
     * Handle timeouts and retransmissions

   * Party Synchronization:
     * Implement barrier synchronization protocol
     * Add heartbeat mechanism for liveness detection
     * Create synchronization points for key protocol phases
     * Handle party failures and timeouts

   * Preprocessing Management:
     * Create PreprocessingManager class
     * Implement secure storage for preprocessing data
     * Add preprocessing data validation
     * Handle preprocessing data distribution

5. Security Considerations:
   * Use TLS 1.3 for secure channels
   * Implement proper key derivation for each session
   * Add message authentication codes
   * Validate all received data
   * Implement proper error handling for security violations

6. Implementation Strategy:
   * Phase 1: Network Infrastructure
     * Set up WebSocket server/client infrastructure
     * Implement secure channel establishment
     * Add basic message handling

   * Phase 2: Share Distribution
     * Implement secure share transfer
     * Add share verification
     * Handle distribution errors

   * Phase 3: Synchronization
     * Add barrier synchronization
     * Implement heartbeat mechanism
     * Handle party failures

   * Phase 4: Preprocessing
     * Create preprocessing data management
     * Implement secure storage
     * Add data validation

### High-level Task Breakdown

1. Environment Setup Phase:
   * Install MP-SPDZ system dependencies (automake, build-essential, clang, etc.)
   * Clone and compile MP-SPDZ from source
   * Set up development environment with proper paths
   * Configure protocol parameters and security settings

2. TypeScript Integration Phase:
   * Create TypeScript bindings for MP-SPDZ core functionality
   * Implement protocol selection and initialization
   * Add proper type definitions for all MP-SPDZ interfaces
   * Set up Node.js process management for MP-SPDZ binaries

3. Protocol Implementation Phase:
   * Implement preprocessing phase management
   * Add support for MASCOT protocol (default secure protocol)
   * Implement SPDZ2k for integer arithmetic
   * Add Semi2k protocol for better performance in semi-honest setting
   * Create proper party management and coordination

4. Security Features Phase:
   * Implement secure key generation and management
   * Add proper audit logging and verification
   * Set up secure channel communication
   * Implement proper error handling and recovery

5. Migration Phase:
   * Create adapter layer for backward compatibility
   * Migrate existing computations to MP-SPDZ
   * Update dependent components
   * Remove JIFF dependency

### Current Status / Progress Tracking

* Initial analysis complete
* Selected MP-SPDZ as replacement for JIFF
* System dependencies installed and verified
* MP-SPDZ repository cloned and configured
* Dependencies configuration progress complete
* Compilation completed
* TypeScript Integration Progress complete
* MASCOT Protocol Implementation complete
* SPDZ2k Protocol Implementation complete
* Semi2k Protocol Implementation complete
* Project Structure complete
* Integration Test Suite Implementation complete
* Performance Optimization Implementation complete:
  * Created performance profiling script in `scripts/performance-profile.ts`
  * Added test cases for various operations:
    * Small and large integer sharing
    * Integer multiplication
    * Integer comparison
    * Bitwise XOR
  * Implemented metrics collection:
    * Operation timing (average, min, max)
    * Memory usage tracking
    * Protocol-specific metrics
  * Added performance report generation:
    * Operation-wise summaries
    * Protocol comparisons
    * Optimization recommendations
  * Created runner script `scripts/run-performance-profile.sh`
  * Added proper cleanup and resource management
* Documentation Progress:
  * API documentation complete
  * Deployment guide complete:
    * System requirements and dependencies documented
    * Installation instructions provided
    * Configuration guidelines established
    * Docker and Kubernetes deployment templates created
    * Monitoring and logging setup documented
    * Troubleshooting guide completed
    * Performance tuning recommendations added
    * Security guidelines documented
* Release Preparation:
  * Created CHANGELOG.md with version 1.0.0 details
  * Created comprehensive RELEASE_NOTES.md:
    * Feature overview
    * Migration guide from JIFF
    * Installation instructions
    * Performance considerations
    * Security recommendations
    * Known issues and future plans
    * Support and contribution guidelines

### Next Steps and Action Items

1. Final Review (Priority: High)
   * Review all documentation for accuracy
   * Verify all code examples work
   * Test migration guide steps
   * Check all links and references

2. Release Process (Priority: High)
   * Create git tag for v1.0.0
   * Prepare npm package
   * Update package.json
   * Verify package contents

Implementation Order:

1. Complete final documentation review
2. Test package installation
3. Create release tag
4. Publish to npm

The Executor should focus on implementing the SPDZ2k protocol first, as it's crucial for integer arithmetic operations. Integration tests should be developed alongside to ensure proper functionality with the actual MP-SPDZ binaries.

### Executor's Feedback or Assistance Requests

The MASCOT protocol implementation is now complete with all core functionality implemented. The next steps should focus on:

1. Implementing the SPDZ2k protocol for integer arithmetic
2. Adding the Semi2k protocol for semi-honest settings
3. Creating the adapter layer for backward compatibility with JIFF

Would like Planner's input on setting up the integration test environment with actual MP-SPDZ binaries, particularly regarding:

1. Test environment configuration
2. Binary compilation settings
3. Network setup for multi-party testing
4. Performance benchmarking parameters

### Dependencies and Requirements

1. System Dependencies:
   * automake
   * build-essential
   * clang
   * cmake
   * git
   * libboost-dev
   * libgmp-dev
   * libntl-dev
   * libsodium-dev
   * libssl-dev
   * libtool
   * python3

2. Development Requirements:
   * TypeScript 5.0+
   * Node.js 22+
   * C++17 compatible compiler
   * Python 3.8+

3. Security Requirements:
   * Proper key generation and management
   * Secure channel communication
   * Audit logging
   * Error handling and recovery
   * Protocol-specific security parameters

### Timeline and Milestones

Week 1:

* Set up MP-SPDZ environment
* Install dependencies
* Basic TypeScript bindings

Week 2:

* Implement MASCOT protocol support
* Add preprocessing phase management
* Create party coordination

Week 3:

* Add remaining protocol support
* Implement security features
* Begin migration

Week 4:

* Complete migration
* Testing and validation
* Documentation and deployment

### Lessons 2

* Always verify protocol security requirements before implementation
* Use proper preprocessing phase management for MPC protocols
* Implement secure channel communication from the start
* Handle protocol-specific parameters carefully
* Document security assumptions and guarantees
* Test with multiple parties and security settings
* Deployment Best Practices:
  * Use Docker for consistent environments
  * Implement proper monitoring from the start
  * Configure appropriate resource limits
  * Set up proper logging and metrics collection
  * Document deployment procedures thoroughly
  * Provide troubleshooting guides
  * Include security hardening steps
  * Test in production-like environments
  * Validate all configuration parameters
  * Monitor system resource usage
